#  ğŸ§  Debate-Based Claim Analysis System 

A multi-stage NLP pipeline that:

Extracts factual claims from input text

Simplifies claims using LLM-based rewriting

Enriches entities with Wikipedia definitions

Classifies debatability using a hybrid AI + rule-based architecture

Provides an interactive Gradio UI

## ğŸš€ Project Overview

This project implements a claim-first NLP pipeline architecture designed for structured argument analysis.

Unlike simple text classifiers, this system:

Breaks a paragraph into atomic factual claims

Simplifies each claim independently

Detects whether each claim is debatable

Uses multiple reasoning layers for robustness

It is built for research, experimentation, and future expansion into argument mining or balanced debate systems.

## ğŸ—ï¸ Architecture
```
Input Paragraph
      â”‚
      â–¼
Module 1: Claim Extraction (Flan-T5)
      â”‚
      â–¼
Module 2: Claim Simplification + Wikipedia Enrichment
      â”‚
      â–¼
Module 3: Debatability Classification
      â”‚
      â”œâ”€â”€ Layer 1: Authoritative factual override
      â”œâ”€â”€ Layer 2: Gemini 2.5 Flash (semantic reasoning)
      â”œâ”€â”€ Layer 3: Rule-based fallback
      â””â”€â”€ Layer 4: HuggingFace zero-shot fallback
      â”‚
      â–¼
Module 4: Evidence Retrieval
        â”‚
        â”œâ”€â”€ Web Search (DDGS)
        â”œâ”€â”€ Scrape HTML (requests + BeautifulSoup)
        â”œâ”€â”€ Remove boilerplate
        â”œâ”€â”€ Extract meaningful paragraphs
        â”œâ”€â”€ Deduplicate URLs
        â””â”€â”€ Return raw evidence chunks
        â”‚
        â–¼
Module 5 (Future): Stance Classification
Gradio UI Output
```
## ğŸ“¦ Modules Implemented
### ğŸ”¹ Module 1 â€“ Claim Extraction

File: module1_claim_extraction.py

Uses google/flan-t5-small

Splits paragraph into sentences (NLTK)

Extracts factual claims from each sentence

Applies:

Numeric consistency checks

Duplicate filtering

Minimum content validation

Output format:
```
[
  {
    "claim_id": 1,
    "claim": "India's GDP grew by 7.2% in 2022."
  }
]
```
### ğŸ”¹ Module 2 â€“ Claim Simplification + Entity Enrichment

File: module2_claim_simplification.py

Features:

Rewrites claims using Flan-T5

Detects named entities via SpaCy

Fetches short Wikipedia summaries

Injects entity definitions inline

Example:
```
Original:
Barack Obama served as the 44th President of the United States.

Simplified:
Barack Obama (44th President of the United States from 2009â€“2017) served as the 44th President of the United States.
```
### ğŸ”¹ Module 3 â€“ Debatability Classification

File: module3_debatability_detection.py

Hybrid layered architecture:

#### ğŸ¥‡ Layer 1 â€“ Authoritative Override

Automatically marks as non-debatable if:

Contains verified numerical data + year

Mentions official institutional sources

#### ğŸ¥ˆ Layer 2 â€“ Gemini 2.5 Flash (Primary AI Layer)

Uses:

google-genai SDK
Model: gemini-2.5-flash


Determines if:

The claim is open to reasonable disagreement

Or purely factual

#### ğŸ¥‰ Layer 3 â€“ Rule-Based Fallback

Detects:

Attribution markers (experts argue, critics say)

Modality (could, might, expected to)

#### ğŸ Layer 4 â€“ HuggingFace Zero-Shot Fallback

Model:

typeform/distilbert-base-uncased-mnli


Used only if previous layers fail.

### ğŸ”¹ Module 4 â€“ Evidence Retrieval (Web Search + Scraping)

File: module4_webscraping.py

#### ğŸ¯ Purpose

Module 4 performs pure evidence retrieval for debatable claims.

It does NOT:

Classify stance (that is Module 5â€™s job)

Generate arguments

Summarize content

It ONLY:

Searches the web

Scrapes relevant article content

Cleans and filters noise

Deduplicates sources

Returns structured evidence chunks

This separation keeps the pipeline modular and scalable.

#### ğŸŒ Retrieval Strategy

For each debatable claim:

ğŸ” Search 5 pro-leaning results

ğŸ” Search 5 opposing-leaning results

Scrape paragraphs from all 10 sources

Return them together (not separated)

âš ï¸ Module 4 does NOT label them as pro/anti.

### ğŸ›  Technologies Used

ddgs (DuckDuckGo search API)

requests

BeautifulSoup

Custom boilerplate filtering

URL validation (PDF & academic site blocking)

### ğŸ§¹ Cleaning Logic

Module 4 removes:

Script/style tags

Boilerplate phrases (subscribe, privacy policy, etc.)

Very short paragraphs (< 100 characters)

Duplicate URLs

Blocked domains (ResearchGate, ScienceDirect)

PDF links

This ensures high-quality evidence chunks for downstream stance modeling.

### âš™ï¸ Design Philosophy

Module 4 is intentionally:

ğŸ”¹ Retrieval-only

ğŸ”¹ Model-agnostic

ğŸ”¹ Gemini-free

ğŸ”¹ No stance bias

ğŸ”¹ RAG-ready

This ensures:

Transparency

Scalability

Clean separation of concerns

Compatibility with local LLMs (Mistral, LLaMA, etc.)
### ğŸ“¤ Output Format
```
[
  {
    "claim_id": 1,
    "claim": "Artificial intelligence will replace most human jobs within the next 20 years",
    "label": "debatable",
    "evidence_chunks": [
        {
            "source": "CNN â€“ AI replace human workers",
            "url": "https://...",
            "content": "Paragraph text..."
        },
        {
            "source": "Brookings â€“ AI and inequality",
            "url": "https://...",
            "content": "Paragraph text..."
        }
    ]
  }
]
```

### ğŸ–¥ï¸ User Interface

File: interface.py

Built using:

Gradio Blocks


Displays:

Extracted Claims

Simplified Claims

Debatability Classification


Launch:
```
python interface.py
```
## ğŸ”§ Installation Guide
#### 1ï¸âƒ£ Create Virtual Environment
```
python -m venv gpuenv
```

Activate:

Windows (CMD):
```
gpuenv\Scripts\activate
```
#### 2ï¸âƒ£ Install Dependencies
```
pip install torch transformers nltk spacy wikipedia-api requests gradio google-genai
python -m spacy download en_core_web_sm
```
#### 3ï¸âƒ£ Set Gemini API Key

In CMD:
```
set GEMINI_API_KEY=YOUR_KEY_HERE
```

Verify:
```
echo %GEMINI_API_KEY%
```
#### 4ï¸âƒ£ Verify Gemini API

Create verify_gemini.py:
```
from google import genai
import os

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

response = client.models.generate_content(
    model="gemini-2.5-flash",
    contents="Reply with only hello"
)

print(response.text)

```
Run:
```
python verify_gemini.py
```

Expected output:
```
hello
```